--- a/third_party/python/glean_parser/glean_parser/util.py
+++ b/third_party/python/glean_parser/glean_parser/util.py
@@ -265,7 +265,7 @@ def fetch_remote_url(url: str, cache: bo
             if key in dc:
                 return dc[key]
 
-    contents: str = urllib.request.urlopen(url).read()
+    contents = urllib.request.urlopen(url).read()
 
     if cache:
         with diskcache.Cache(cache_dir) as dc:
--- a/third_party/python/importlib_metadata/importlib_metadata/__init__.py
+++ b/third_party/python/importlib_metadata/importlib_metadata/__init__.py
@@ -155,7 +155,7 @@ class EntryPoint(
     following the attr, and following any extras.
     """
 
-    dist: Optional['Distribution'] = None
+    dist = None
 
     def load(self):
         """Load the entry point from its definition. If only a module
--- a/third_party/python/glean_parser/glean_parser/lint.py
+++ b/third_party/python/glean_parser/glean_parser/lint.py
@@ -264,9 +264,7 @@ def check_expired_metric(
 
 # The checks that operate on an entire category of metrics:
 #    {NAME: (function, is_error)}
-CATEGORY_CHECKS: Dict[
-    str, Tuple[Callable[[str, Iterable[metrics.Metric]], LintGenerator], CheckType]
-] = {
+CATEGORY_CHECKS = {
     "COMMON_PREFIX": (check_common_prefix, CheckType.error),
     "CATEGORY_GENERIC": (check_category_generic, CheckType.error),
 }
@@ -274,9 +272,7 @@ CATEGORY_CHECKS: Dict[
 
 # The checks that operate on individual metrics:
 #     {NAME: (function, is_error)}
-METRIC_CHECKS: Dict[
-    str, Tuple[Callable[[metrics.Metric, dict], LintGenerator], CheckType]
-] = {
+METRIC_CHECKS = {
     "UNIT_IN_NAME": (check_unit_in_name, CheckType.error),
     "BUG_NUMBER": (check_bug_number, CheckType.error),
     "BASELINE_PING": (check_valid_in_baseline, CheckType.error),
@@ -289,9 +285,7 @@ METRIC_CHECKS: Dict[
 
 # The checks that operate on individual pings:
 #     {NAME: (function, is_error)}
-PING_CHECKS: Dict[
-    str, Tuple[Callable[[pings.Ping, dict], LintGenerator], CheckType]
-] = {
+PING_CHECKS = {
     "BUG_NUMBER": (check_bug_number, CheckType.error),
 }
 
@@ -314,7 +308,7 @@ def _lint_pings(
     category: Dict[str, Union[metrics.Metric, pings.Ping]],
     parser_config: Dict[str, Any],
 ):
-    nits: List[GlinterNit] = []
+    nits = []
 
     for (ping_name, ping) in sorted(list(category.items())):
         assert isinstance(ping, pings.Ping)
@@ -350,7 +344,7 @@ def lint_metrics(
     if parser_config is None:
         parser_config = {}
 
-    nits: List[GlinterNit] = []
+    nits = []
     for (category_name, category) in sorted(list(objs.items())):
         if category_name == "pings":
             nits.extend(_lint_pings(category, parser_config))
@@ -423,7 +417,7 @@ def lint_yaml_files(
 
     # Generic type since the actual type comes from yamllint, which we don't
     # control.
-    nits: List = []
+    nits = []
     for path in input_filepaths:
         if not path.is_file() and parser_config.get("allow_missing_files", False):
             continue
--- a/third_party/python/glean_parser/glean_parser/metrics.py
+++ b/third_party/python/glean_parser/glean_parser/metrics.py
@@ -33,10 +33,10 @@ class DataSensitivity(enum.Enum):
 
 
 class Metric:
-    typename: str = "ERROR"
-    glean_internal_metric_cat: str = "glean.internal.metrics"
-    metric_types: Dict[str, Any] = {}
-    default_store_names: List[str] = ["metrics"]
+    typename = "ERROR"
+    glean_internal_metric_cat = "glean.internal.metrics"
+    metric_types = {}
+    default_store_names = ["metrics"]
 
     def __init__(
         self,
--- a/third_party/python/glean_parser/glean_parser/pings.py
+++ b/third_party/python/glean_parser/glean_parser/pings.py
@@ -56,7 +56,7 @@ class Ping:
         # _validated indicates whether this metric has already been jsonschema
         # validated (but not any of the Python-level validation).
         if not _validated:
-            data: Dict[str, util.JSONType] = {
+            data = {
                 "$schema": parser.PINGS_ID,
                 self.name: self._serialize_input(),
             }
--- a/third_party/python/glean_parser/glean_parser/parser.py
+++ b/third_party/python/glean_parser/glean_parser/parser.py
@@ -84,7 +84,7 @@ def _load_file(
     if not isinstance(schema_key, str):
         raise TypeError("Invalid schema key {}".format((schema_key)))
 
-    filetype: Optional[str] = None
+    filetype = None
     try:
         filetype = schema_key.split("/")[-2]
     except IndexError:
@@ -379,8 +379,8 @@ def parse_objects(
     if config is None:
         config = {}
 
-    all_objects: ObjectTree = OrderedDict()
-    sources: Dict[Any, Path] = {}
+    all_objects = OrderedDict()
+    sources = {}
     filepaths = util.ensure_list(filepaths)
     for filepath in filepaths:
         content, filetype = yield from _load_file(filepath, config)
--- a/third_party/python/glean_parser/glean_parser/coverage.py
+++ b/third_party/python/glean_parser/glean_parser/coverage.py
@@ -30,7 +30,7 @@ def _outputter_codecovio(metrics: Object
         `_annotate_coverage`.
     :param output_path: The file to output to.
     """
-    coverage: Dict[str, List] = {}
+    coverage = {}
     for category in metrics.values():
         for metric in category.values():
             defined_in = metric.defined_in
--- a/third_party/python/glean_parser/glean_parser/markdown.py
+++ b/third_party/python/glean_parser/glean_parser/markdown.py
@@ -204,8 +204,8 @@ def output_markdown(
     # }
     #
     # This also builds a dictionary of custom pings, if available.
-    custom_pings_cache: Dict[str, pings.Ping] = defaultdict()
-    metrics_by_pings: Dict[str, List[metrics.Metric]] = defaultdict(list)
+    custom_pings_cache = defaultdict()
+    metrics_by_pings = defaultdict(list)
     for _category_key, category_val in objs.items():
         for obj in category_val.values():
             # Filter out custom pings. We will need them for extracting
--- a/third_party/python/glean_parser/glean_parser/translate.py
+++ b/third_party/python/glean_parser/glean_parser/translate.py
@@ -70,7 +70,7 @@ def transform_metrics(objects):
     external.
     """
     counters = {}
-    numerators_by_denominator: Dict[str, Any] = {}
+    numerators_by_denominator = {}
     for category_val in objects.values():
         for metric in category_val.values():
             fqmn = metric.identifier()
--- a/xpcom/ds/tools/incremental_dafsa.py
+++ b/xpcom/ds/tools/incremental_dafsa.py
@@ -14,12 +14,6 @@ from typing import List, Dict, Optional,
 
 
 class Node:
-    children: Dict[str, "Node"]
-    parents: Dict[str, List["Node"]]
-    character: str
-    is_root_node: bool
-    is_end_node: bool
-
     def __init__(self, character, is_root_node=False, is_end_node=False):
         self.children = {}
         self.parents = {}
@@ -156,9 +150,6 @@ class Node:
 
 
 class SuffixCursor:
-    index: int  # Current position of the cursor within the DAFSA.
-    node: Node
-
     def __init__(self, index, node):
         self.index = index
         self.node = node
@@ -229,15 +220,6 @@ class DafsaAppendStateMachine:
     The next suffix node we'll attempt to find is at index "9".
     """
 
-    root_node: Node
-    prefix_index: int
-    suffix_cursor: SuffixCursor
-    stack: List[Node]
-    word: str
-    suffix_overlaps_prefix: bool
-    first_fork_index: Optional[int]
-    _state: Callable
-
     def __init__(self, word, root_node, end_node):
         self.root_node = root_node
         self.prefix_index = 0
@@ -475,9 +457,6 @@ def _duplicate_fork_nodes(stack, fork_in
 
 
 class Dafsa:
-    root_node: Node
-    end_node: Node
-
     def __init__(self):
         self.root_node = Node(None, is_root_node=True)
         self.end_node = Node(None, is_end_node=True)
